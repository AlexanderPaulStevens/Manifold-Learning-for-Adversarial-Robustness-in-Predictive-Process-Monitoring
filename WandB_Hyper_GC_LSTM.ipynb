{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1329,"status":"ok","timestamp":1686924270344,"user":{"displayName":"Alexander Stevens","userId":"06895933540823444481"},"user_tz":-120},"id":"xlJMQYebUo-X","outputId":"5332d5e4-9d66-4d67-d1c3-38ce3dc27f2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686924270344,"user":{"displayName":"Alexander Stevens","userId":"06895933540823444481"},"user_tz":-120},"id":"9FU_59Cc9wyx","outputId":"c4e9d75f-31b7-43b1-9a04-84b5d006637d"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/CurrentWork/Manifold/AdversarialRobustnessGeneralization\n"]}],"source":["%cd /content/drive/MyDrive/CurrentWork/Manifold/AdversarialRobustnessGeneralization"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16116,"status":"ok","timestamp":1686924286458,"user":{"displayName":"Alexander Stevens","userId":"06895933540823444481"},"user_tz":-120},"id":"RdtQRWXV-pSo","outputId":"2d15c2cf-8048-4dbd-b383-7afb52c8fd61"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: wandb==0.13.4 in /usr/local/lib/python3.10/dist-packages (0.13.4)\n","Requirement already satisfied: Click!=8.0.0,\u003e=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.4) (8.1.3)\n","Requirement already satisfied: GitPython\u003e=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.4) (3.1.31)\n","Requirement already satisfied: requests\u003c3,\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.4) (2.27.1)\n","Requirement already satisfied: promise\u003c3,\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.4) (2.3)\n","Requirement already satisfied: shortuuid\u003e=0.5.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.4) (1.0.11)\n","Requirement already satisfied: psutil\u003e=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.4) (5.9.5)\n","Requirement already satisfied: sentry-sdk\u003e=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.4) (1.25.1)\n","Requirement already satisfied: six\u003e=1.13.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.4) (1.16.0)\n","Requirement already satisfied: docker-pycreds\u003e=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.4) (0.4.0)\n","Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,\u003c5,\u003e=3.12.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.4) (3.20.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.4) (6.0)\n","Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.4) (0.1.2)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.4) (1.3.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.4) (67.7.2)\n","Requirement already satisfied: gitdb\u003c5,\u003e=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython\u003e=1.0.0-\u003ewandb==0.13.4) (4.0.10)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.0.0-\u003ewandb==0.13.4) (1.26.15)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.0.0-\u003ewandb==0.13.4) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.0.0-\u003ewandb==0.13.4) (2.0.12)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.0.0-\u003ewandb==0.13.4) (3.4)\n","Requirement already satisfied: smmap\u003c6,\u003e=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb\u003c5,\u003e=4.0.1-\u003eGitPython\u003e=1.0.0-\u003ewandb==0.13.4) (5.0.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.10/dist-packages (7.352.0)\n"]}],"source":["%pip install wandb==0.13.4\n","%pip install nvidia-ml-py3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"hWtZI9wSJCb4"},"outputs":[{"name":"stdout","output_type":"stream","text":["path params_dir_DL\n","Dataset: bpic2015_1_f2\n","Responsible_actor                                       19\n","SUMleges                                               311\n","Case ID                                                696\n","label                                                    2\n","Aanleg (Uitvoeren werk of werkzaamheid)                  2\n","Bouw                                                     2\n","Brandveilig gebruik (melding)                            2\n","Brandveilig gebruik (vergunning)                         2\n","Gebiedsbescherming                                       2\n","Handelen in strijd met regels RO                         2\n","Inrit/Uitweg                                             2\n","Kap                                                      2\n","Milieu (melding)                                         2\n","Milieu (neutraal wijziging)                              2\n","Milieu (omgevingsvergunning beperkte milieutoets)        2\n","Milieu (vergunning)                                      2\n","Monument                                                 2\n","Reclame                                                  2\n","Sloop                                                    2\n","Activity                                               380\n","monitoringResource                                      22\n","question                                                11\n","org:resource                                            20\n","time:timestamp                                       16770\n","timesincemidnight                                      610\n","month                                                   12\n","weekday                                                  7\n","hour                                                    15\n","timesincelastevent                                    4083\n","timesincecasestart                                   16390\n","event_nr                                               101\n","open_cases                                              77\n","dtype: int64\n","Create sweep with ID: 9grhk6mm\n","Sweep URL: https://wandb.ai/adversarial_robustness/Bayes_LSTM/sweeps/9grhk6mm\n"]},{"name":"stderr","output_type":"stream","text":["INFO:wandb.agents.pyagent:Starting sweep agent: entity=adversarial_robustness, project=Bayes_LSTM, count=15\n"]},{"name":"stdout","output_type":"stream","text":["embed_size 32\n","hidden_size 32\n","batch_size 128\n","learning rate 0.006496320018086848\n","Model(\n","  (embed_act): Embedding(242, 32)\n","  (embed_res): Embedding(17, 32)\n","  (lstm1): LSTM(64, 32, batch_first=True)\n","  (fc): Linear(in_features=32, out_features=1, bias=True)\n","  (sigmoid): Sigmoid()\n",")\n","optimizer Nadam\n","training\n","Epoch:  0\n","testing\n","validation_loss 0.61655605\n","Epoch:  1\n","testing\n","validation_loss 0.5679024\n","Epoch:  2\n","testing\n","validation_loss 0.5405635\n","Epoch:  3\n","testing\n","validation_loss 0.52450055\n","Epoch:  4\n","testing\n","validation_loss 0.51560885\n","Epoch:  5\n","testing\n","validation_loss 0.51006263\n","Epoch:  6\n","testing\n","validation_loss 0.5066518\n","Epoch:  7\n","testing\n","validation_loss 0.50446403\n","Epoch:  8\n","testing\n","validation_loss 0.50331914\n","Epoch:  9\n","testing\n","validation_loss 0.5023379\n","Epoch:  10\n","testing\n","validation_loss 0.5017226\n","Epoch:  11\n","testing\n","validation_loss 0.5013082\n","Epoch:  12\n","testing\n","validation_loss 0.50107324\n","Epoch:  13\n","testing\n","validation_loss 0.5008533\n","Epoch:  14\n","testing\n","validation_loss 0.50072503\n","Epoch:  15\n","testing\n","validation_loss 0.5006823\n","Epoch:  16\n"]},{"name":"stderr","output_type":"stream","text":["INFO:root:Current metric value better than 0.5005636215209961 better than best inf, saving model at params_dir_DL/hyper_models/bpic2015_1_f2/Model_0.006496320018086848_32_Nadam_128_epoch16.pt, \u0026 logging model weights to W\u0026B.\n"]},{"name":"stdout","output_type":"stream","text":["testing\n","validation_loss 0.5005636\n","Epoch:  17\n"]},{"name":"stderr","output_type":"stream","text":["INFO:root:Current metric value better than 0.5005161166191101 better than best 0.5005636215209961, saving model at params_dir_DL/hyper_models/bpic2015_1_f2/Model_0.006496320018086848_32_Nadam_128_epoch17.pt, \u0026 logging model weights to W\u0026B.\n"]},{"name":"stdout","output_type":"stream","text":["testing\n","validation_loss 0.5005161\n","Epoch:  18\n"]},{"name":"stderr","output_type":"stream","text":["INFO:root:Current metric value better than 0.5004550814628601 better than best 0.5005161166191101, saving model at params_dir_DL/hyper_models/bpic2015_1_f2/Model_0.006496320018086848_32_Nadam_128_epoch18.pt, \u0026 logging model weights to W\u0026B.\n"]},{"name":"stdout","output_type":"stream","text":["testing\n","validation_loss 0.5004551\n","Epoch:  19\n"]}],"source":["import sys\n","import os\n","import wandb\n","import pandas as pd\n","sys.path.append(os.getcwd())\n","import random\n","import numpy as np\n","import pandas as pd\n","# packages from https://github.com/irhete/predictive-monitoring-benchmark/blob/master/experiments/experiments.py\n","from DatasetManager import DatasetManager\n","from settings import global_setting, model_setting, training_setting\n","from LSTM import Model, CheckpointSaver\n","#user-specified packages\n","from util.DataCreation import DataCreation\n","from util.Arguments import Args\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","import torch\n","import torch.nn as nn\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from sklearn.metrics import roc_auc_score\n","# Use GPU if available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","# to explicitly raise an error with a stack trace to easier debug which operation might have created the invalid values\n","torch.autograd.set_detect_anomaly(True)\n","\n","#hyperopt\n","import hyperopt\n","from hyperopt import hp, Trials, fmin, tpe, STATUS_OK\n","from hyperopt.pyll.base import scope\n","import yaml\n","# set logging\n","import logging\n","logging.getLogger().setLevel(logging.INFO)\n","\n","def seed_worker(worker_id):\n","    worker_seed = torch.initial_seed() % 2**32\n","    np.random.seed(worker_seed)\n","    random.seed(worker_seed)\n","\n","g = torch.Generator()\n","g.manual_seed(0)\n","##################################################################\n","\n","dataset_ref_to_datasets = {\n","    \"production\": [\"production\"],\n","    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(1,6)],\n","    \"bpic2012\": [\"bpic2012_accepted\",\"bpic2012_cancelled\",\"bpic2012_declined\"],\n","    \"hospital_billing\": [\"hospital_billing_%s\"%suffix for suffix in [2,3]],\n","    \"traffic_fines\": [\"traffic_fines_%s\" % formula for formula in range(1, 2)],\n","    # \"bpic2017\": [\"bpic2017_accepted\",\"bpic2017_cancelled\",\"bpic2017_refused\"],\n","    #\"sepsis_cases\": [\"sepsis_cases_2\",\"sepsis_cases_4\"],\n","    #\"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(2,4)],\n","    }\n","\n","dataset_name= 'bpic2015_1_f2'\n","\n","path = 'params_dir_DL'\n","print('path',path)\n","os.environ['WANDB_DIR']= path\n","os.environ[\"WANDB_SILENT\"] = \"true\"\n","\n","dir_path = path+'/hyper_models/'+dataset_name\n","# create results directory\n","if not os.path.exists(os.path.join(dir_path)):\n","    os.makedirs(os.path.join(dir_path))\n","\n","seed = global_setting['seed']\n","epochs = training_setting['epochs']\n","n_splits = global_setting['n_splits']\n","max_evals = global_setting['max_evals']\n","train_ratio = global_setting['train_ratio']\n","clip = training_setting[\"clip\"]\n","\n","project_name = 'Bayes_LSTM'\n","dataset_group = 'LSTM'\n","name = dataset_group + '_' + dataset_name\n","\n","print('Dataset:', dataset_name)\n","\n","dataset_manager = DatasetManager(dataset_name)\n","data = dataset_manager.read_dataset()\n","arguments = Args(dataset_name)\n","cls_encoder_args, min_prefix_length, max_prefix_length, activity_col, resource_col = arguments.extract_args(data, dataset_manager)\n","print(data.nunique())\n","cls_method = 'LSTM'\n","cls_encoding = 'embed'\n","datacreator = DataCreation(dataset_manager, dataset_name, cls_method, cls_encoding)\n","cat_cols = [activity_col, resource_col]\n","no_cols_list = []\n","cols = [cat_cols[0], cat_cols[1], cls_encoder_args['case_id_col'], 'label', 'event_nr', 'prefix_nr']\n","train, val = dataset_manager.split_data_strict(data, train_ratio, split=\"temporal\")\n","train, test = dataset_manager.split_data_strict(train, train_ratio, split=\"temporal\")\n","for i in cat_cols:\n","    _, _, _, no_cols = datacreator.create_indexes(i, train)\n","    no_cols_list.append(no_cols)\n","vocab_size = [no_cols_list[0]+1, no_cols_list[1]+1]\n","# you don't need to do that for the test data, as the prepare inputs is only fitted on the training data\n","# prepare chunks for CV\n","dt_prefixes = []\n","\n","# prefix generation of test data\n","dt_train_prefixes = dataset_manager.generate_prefix_data(train, min_prefix_length, max_prefix_length)\n","dt_val_prefixes = dataset_manager.generate_prefix_data(val, min_prefix_length, max_prefix_length)\n","dt_test_prefixes = dataset_manager.generate_prefix_data(test, min_prefix_length, max_prefix_length)\n","\n","#######WANDB################\n","with open(\"config/LSTM_sweep.yaml\", 'r') as stream:\n","    sweep_config = yaml.safe_load(stream)\n","\n","sweep_config['name'] =  name\n","sweep_id = wandb.sweep(sweep_config, project=project_name, entity=\"adversarial_robustness\")\n","\n","iterations= max_evals\n","\n","train_cat_cols, val_cat_cols, _ = datacreator.prepare_inputs(dt_train_prefixes.loc[:, cat_cols], dt_val_prefixes.loc[:, cat_cols])\n","train_cat_cols, test_cat_cols, _ = datacreator.prepare_inputs(dt_train_prefixes.loc[:, cat_cols], dt_test_prefixes.loc[:, cat_cols])\n","dt_train_prefixes[cat_cols] = train_cat_cols\n","dt_val_prefixes[cat_cols] = val_cat_cols\n","dt_test_prefixes[cat_cols] = test_cat_cols\n","payload_values = {key: list(dt_train_prefixes[key].unique()) for key in cat_cols}\n","del train_cat_cols, test_cat_cols, val_cat_cols\n","\n","dt_train_prefixes_original, dt_test_prefixes_original, dt_val_prefixes_original = dt_train_prefixes.copy(), dt_test_prefixes.copy(), dt_val_prefixes.copy()\n","nr_events_original = list(dataset_manager.get_prefix_lengths(dt_test_prefixes))\n","test_y_original = dataset_manager.get_label_numeric(dt_test_prefixes)\n","train_y_original = dataset_manager.get_label_numeric(dt_train_prefixes)\n","val_y_original = dataset_manager.get_label_numeric(dt_val_prefixes)\n","dt_train_prefixes = dt_train_prefixes[cols].copy()\n","dt_test_prefixes = dt_test_prefixes[cols].copy()\n","dt_val_prefixes = dt_val_prefixes[cols].copy()\n","train_y = dataset_manager.get_label_numeric(dt_train_prefixes)\n","test_y = dataset_manager.get_label_numeric(dt_test_prefixes)\n","val_y = dataset_manager.get_label_numeric(dt_val_prefixes)\n","\n","# groupby case ID\n","ans_train_act = datacreator.groupby_caseID(dt_train_prefixes, cols, activity_col)\n","ans_test_act = datacreator.groupby_caseID(dt_test_prefixes, cols, activity_col)\n","ans_train_res = datacreator.groupby_caseID(dt_train_prefixes, cols, resource_col)\n","ans_test_res = datacreator.groupby_caseID(dt_test_prefixes, cols, resource_col)\n","\n","######ACTIVITY_COL########\n","activity_train = datacreator.pad_data(ans_train_act).to(device)\n","activity_test = datacreator.pad_data(ans_test_act).to(device)\n","\n","# ######RESOURCE COL########\n","resource_train = datacreator.pad_data(ans_train_res).to(device)\n","resource_test = datacreator.pad_data(ans_test_res).to(device)\n","\n","def train(config=None):\n","\t# Initialize a new wandb run\n","    with wandb.init(config=config,project=project_name, group=name, save_code=False) as run:\n","        global resource_train, resource_test, activity_train, activity_test\n","        # If called by wandb.agent, as below,\n","        # this config will be set by Sweep Controller\n","        config = wandb.config\n","        batch_size = config[\"batch_size\"]\n","        hidden_size = config[\"hidden_size\"]\n","        learning_rate = config['learning_rate']\n","        embed_size = training_setting['embed_size']\n","\n","        print('embed_size', embed_size)\n","        print('hidden_size', hidden_size)\n","        print('batch_size', batch_size)\n","        print('learning rate', learning_rate)\n","\n","        ###################MODEL ARCHITECTURE#################################################\n","        # create the input layers and embeddings\n","        dataset = torch.utils.data.TensorDataset(activity_train, resource_train)\n","        dataset = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True, worker_init_fn=seed_worker)\n","\n","        model = Model(vocab_size, embed_size, hidden_size, max_prefix_length).to(device)\n","        print(model)\n","        if config['optimizer']=='RMSprop':\n","            optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n","        elif config['optimizer']=='adam':\n","            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","        elif config['optimizer']=='Nadam':\n","            optimizer = torch.optim.NAdam(model.parameters(), lr=learning_rate)\n","        print('optimizer', config['optimizer'])\n","        scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n","        # checkpoint saver\n","        checkpoint_saver = CheckpointSaver(dirpath=dir_path, decreasing=True, top_n=5)\n","        criterion = nn.BCELoss()\n","        # Define the early stopping patience\n","        print('training')\n","        early_start_patience = 15\n","        early_stop_patience = 60\n","        best_loss = 9999\n","        for epoch in range(epochs):\n","          print(\"Epoch: \", epoch)\n","          for i, (data_act, data_res) in enumerate(dataset, 0): # loop over the data, and jump with step = bptt.\n","                model.train()\n","                data_act = data_act.to(device)\n","                data_res = data_res.to(device)\n","                y_ = model(data_act,data_res).to(device)\n","                train_batch = torch.tensor(train_y[i*batch_size:(i+1)*batch_size], dtype= torch.float32).to(device)\n","                train_batch = train_batch[:, None].to(device)\n","                train_loss = criterion(y_,train_batch).to(device)\n","                wandb.log({\"train_loss\": train_loss})\n","                train_loss.backward()\n","                #torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","          with torch.no_grad():\n","            model.eval()\n","            #mask = ((activity_test != 0) | (resource_test != 0)).any(dim=1).float()\n","            print('testing')\n","            pred = model(activity_test, resource_test).squeeze(-1).to(device)\n","\n","            validation_loss = criterion(pred, torch.tensor(test_y , dtype= torch.float32).to(device))\n","            validation_loss = validation_loss.to('cpu').detach().numpy()\n","            scheduler.step(validation_loss)\n","            # Log evaluation metrics to WandB\n","            wandb.log({\"validation_loss\":validation_loss})\n","            print('validation_loss',validation_loss)\n","            if epoch \u003e early_start_patience:\n","              checkpoint_saver(model, epoch, validation_loss, learning_rate, hidden_size, config['optimizer'], batch_size)\n","            if validation_loss \u003c best_loss:\n","              best_loss = validation_loss\n","          if epoch \u003e early_stop_patience and validation_loss \u003e best_loss:\n","                print('best loss', best_loss)\n","                print('validation loss', validation_loss)\n","                print(\"Early stopping triggered.\")\n","                break\n","        # Empty CUDA cache\n","        torch.cuda.empty_cache()\n","\n","wandb.agent(sweep_id, function= train, count=15, project=project_name, entity=\"adversarial_robustness\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AoLDMqjeAkpN"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0hKpPPtkqD2i"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"","provenance":[{"file_id":"1T4TDFiBLhn1dDUpT4aT5EbmEm2bJO1-l","timestamp":1686568958392},{"file_id":"1GoLqRktSrCp4AGjnALvhOUcHzNrfe3kT","timestamp":1677780759232}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}